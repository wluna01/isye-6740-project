\documentclass{article}
\usepackage{hyperref}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage[justification=centering]{caption}
\usepackage{amsmath,amsfonts,amsthm,fullpage,amssymb,algorithm,color,mathtools,float,hyperref,subcaption,url,graphicx, algorithmic}
\usepackage[document]{ragged2e}
\newcommand{\largeimagewidth}{350}
\newcommand{\mediumimagewidth}{250}
\setlength{\parskip}{1em}
\begin{document}
\begin{titlepage}
\clearpage\thispagestyle{empty}
\centering
\vspace{1cm}

\rule{\linewidth}{1mm} \\[0.5cm]
{ \Large \bfseries ISYE 6740 - Fall 2023\\[0.2cm]
Final Project}\\[0.5cm]
\rule{\linewidth}{1mm} \\[1cm]

\begin{tabular}{l p{5cm}}
\textbf{Team Member Names:} William Luna (Solo) gtID: 903947280 &   \\[10pt]
\textbf{Project Title:} Speaker Attribution in Written Dialogue &  \\[10pt]
\end{tabular} 

\tableofcontents
\newpage

\section{Problem Statement}

\justifying{Effective dialogue in storytelling requires that each character speak distinctly. The purpose of having multiple characters in a story is to provide a diversity of opinions, perspectives, and backgrounds. If every sentence spoken feels appropriate coming out of the mouth of any character, why have multiple characters at all? How bland would the adventures of Harry, Ron, and Hermione be if all three were equal parts courageous, loyal, and inquisitive, instead of each personality compensating for the other two? Would Pride and Prejudice be effective at exposing class inequalities in Victorian society if every character sounded equally posh?}

\justifying{However, empirically evaluating the distinctness of each character's dialogue in a story is a subjective and challenging process.}

\justifying{This project proposes a methodology to evaluate the distinctness between sets of dialogue in terms of vocabulary and style. Then, through applying this methodology to dialogue in television shows and films, assesses the ability to predict either a) the speaker within a piece of media, where the corpus is a single piece of media, or b) from which media a piece of dialogue originated, where the corpus spans several pieces of media . This distinction will be referred to as $Within-Media$ and $Across-Media$ Classifications going forward.}

\justifying{We will conclude with a discussion of the success of the models at speaker/media classification, how they may be applied to evaluate whether character dialogue has a sufficiently unique voice, and areas of future exploration.}

\section{Data Collection}

The corpus for this project was curated from several data sources that provide dialogue from popular movies, television shows, and books:

- \href{https://www.kaggle.com/datasets/blessondensil294/friends-tv-series-screenplay-script/data?select=S01E01+Monica+Gets+A+Roommate.txt}{Friends}: kaggle.com/datasets/blessondensil294/friends-tv-series-screenplay-script/
 
- \href{https://www.kaggle.com/datasets/pierremegret/dialogue-lines-of-the-simpsons}{The Simpsons}: kaggle.com/datasets/pierremegret/dialogue-lines-of-the-simpsons

- \href{https://www.kaggle.com/datasets/paultimothymooney/lord-of-the-rings-data?select=lotr_scripts.csv}{The Lord of the Rings}: kaggle.com/datasets/paultimothymooney/lord-of-the-rings-data

- \href{https://www.kaggle.com/datasets/andradaolteanu/rickmorty-scripts}{Rick and Morty}: kaggle.com/datasets/andradaolteanu/rickmorty-scripts

- \href{https://scriptline.livejournal.com/71215.html#cutid6}{Pride and Prejudice, Downtown Abbey*}: scriptline.livejournal.com/71215.html

After data cleaning, the outcome is a corpus of the following format. In order to make visualizations easier to interpret, and remove characters with too little data, the dialogue from only the six most frequently occurring characters from each media are used:

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{} \textbf{media} & \textbf{speaker} & \textbf{dialogue} \\
        \hline
        Friends & Phoebe &  ``I asked for the news, not the weather.'' \\
        Friends & Phoebe &  ``You'll see. You'll all see.'' \\
        Friends & Joey &  ``How you doin'?'' \\
        Pride and Prejudice & Mr. Darcy &  ``My affections and wishes have not changed, but...''  \\
        The Lord of the Rings & Gollum &  ``My precious!'' \\
        \hline
    \end{tabular}
    \caption{Sample Rows from Dialogue Speaker Corpus}
    \label{tab:images}
\end{table}

\section{Methodology}
\subsection{Feature Engineering}
\subsubsection{Heuristics}
There are multiple frameworks for evaluating the uniqueness between two sets of text. Two were chosen to serve as heuristics for this research–Yule's K and Simpson's D.

\textbf{Yule's K} is a calculation that estimates the variety of vocabulary and in a text^1:

Given:

- $N$ as the total number of words in a text

- $V(N)$ as the number of distinct words

- $V(m, N)$ as the number of words appearing m times in the text

- ${m_{\text{max}}}$ as the largest frequency of a word,

it is defined as:

\[
S_1 = \sum_{m} mV(m, N), S_2 = \sum_{m} m^2V(m, N)
\]

\[
K = C^2 \frac{S_2 - S_1}{S_1^2} = C \left[ - \frac{1}{N} + \sum_{m=1}^{m_{\text{max}}} V(m, N) \left( \frac{m}{N} \right)^2 \right]
\]

\textbf{Simpson's D} (formally Simpson's Diversity Index) was created as a measure of diversity within an ecological community^2. However, it can also be applied as a measure of variety in a text corpus.

Given:

- $i$ as a vector of all the distinct words in the corpus

- $n_i$ as the number of times word $i$ occurs in the corpus

- $N$ as the total number of words in the corpus

it is defined as:

\[
D = \frac{\sum_{i=1}^{n} n_i(n_i - 1)}{N(N - 1)}
\]

Treating the dialogue of each speaker as its own corpus, we calculate Yule's K and Simpson's D for each, plotting the variation:

\begin{figure}[H]
\centering
\includegraphics[width=\largeimagewidth]{images/heuristics.png}
\caption{Yule's K, a heuristic created specifically to measure text, exposes more variability in dialogue.}
\end{figure}

Despite Yule's K's separation of characters in The Simpsons and Friends compared to the other media, it does not appear likely that these heuristics would be sufficient classifying from which piece of media a speaker originated.

However, when comparing speakers within the same media, simple patterns can emerge. In the charts below, characters co-locate on the scatterplot based on their function in the story. Frodo and Sam spend much of the Lord of the Rings traveling together, as do Merry and Pippin. Rick and Morty are the protagonists of the eponymous show, while Beth, Jerry, and Summer are the supporting cast:

\begin{figure}[H]
    \centering
    \caption{Selected Within-Media Plots}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/The Lord of the Rings_heuristics.png}
        \caption{The Lord of the Rings}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Rick and Morty_heuristics.png}
        \caption{Rick and Morty}
        \label{fig:subfig2}
    \end{subfigure}
    \label{fig:main}
\end{figure} 

One important takeaway from this heuristic analysis is that for the media in the corpus, the differences in dialogue across each media (TV show, film, book) are greater than the differences of dialogue between each character within the same piece of media.

\subsubsection{Stylometry}

Stylometry is a term I was not aware of before starting this project. While it's current technical definition is "the statistical analysis of variations in literary style," for this section, its easier to use Augustus de Morgen's original 1851 definition as "if one text 'deals in longer words' than another"^3.

To prepare the data for this section, all dialogue for each speaker was converted into a distributions of sentence and word length. In other words, how many words did a speaker use in each sentence, and how many characters did they use in each word?

The Lord of the Rings is shown here as an illustrative example of all the within-media plots. Most look similar, where despite the fact that one speaker may have a unique distribution (Gandalf below), the majority of the characters' are extremely similar.

However, as we see in the chart on the right., the variance of the distributions are greater across-media:

\begin{figure}[H]
    \centering
    \caption{Within- and Across- Media Sentence Length Distributions}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/The Lord of the Rings_sentence_length.png}
        \caption{The Lord of the Rings}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/overall_sentence_length.png}
        \caption{Overall}
        \label{fig:subfig2}
    \end{subfigure}
    \label{fig:main}
\end{figure} 

Plotting both within- and across-media Kernel Density Estimators (KDEs) $f(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)$,

It is unclear whether the variation in sentence and word length is sufficient to provide predictive features to a classifier:

\begin{figure}[H]
    \centering
    \caption{Selected Within-Media KDE Plots}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/3d_kde_test_0.png}
        \caption{Pride and Prejudice}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/3d_kde_test_1.png}
        \caption{Friends}
        \label{fig:subfig2}
    \end{subfigure}
    \label{fig:main}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[width=\largeimagewidth]{images/3d_kde_test_final.png}
\caption{Across-media KDE Plots. Media with more sophisticated dialogue (Pride and Prejudice, Downtown Abbey) have wider word- and sentence-length distributions than American Sitcoms (The Simpsons, Friends).}
\end{figure}

To assess the potential value of incorporating stylometry into a classifier model, A KDE classifier was created to predict in which media a given line of dialogue was spoken. Given that the accuracy of the model is near-random (approx. 15 percent), and the evidence that dialogue variation is greater across media than within media, I concluded that stylometry, at least at this level of rudimentary implementation, was unlikely to contribute to an effective classifier.

\subsubsection{Vocabulary}

Vocabulary is more commonly used to train text classifiers than stylometry. In NLP, a common ways of creating features from text vocabulary is \textbf{Term Frequency-Inverse Document Frequency}.

My own simple, imprecise definition is: ``how much more or less does this word occur in this text sample compared to how often it occurs in a larger corpora?''

Where the precise definition follows:
\[
\text{TF}(t, d) = \frac{\text{Number of times term } t \text{ appears in a document } d}{\text{Total number of terms in the document}}
\]
\[
\text{IDF}(t, D) = \log \left( \frac{\text{Number of documents}}{\text{Number of documents containing term } t} \right)
\]
\[
\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
\]

It's important to note that the definition of a ``document'' depends on whether an Across- or Within- Media analysis is being performed. For Across-Media, the full corpus of each show or film represents a document, where for Within-Media, all the dialogue for a specific speaker represents a document. In both cases, the result of performing TF-IDF is a matrix of dimensions $NxM$, where $N$ is the number of documents and $M$ is the number of terms in a document. Since many media have vocabulary that is not used in any other piece of media, we should expect the result to be a sparse matrix.

Before training a classifier, K-means Clustering was performed on the top two principal components of the TF-IDF matrix. The resulting scatter plot and low mis-classification rate indicates that vocabulary provides more effective predictor variables than those explored above:

\begin{figure}[H]
    \centering
    \caption{PCA Assessment of TF-IDF}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/k_means_across_media_misclassifications.png}
        \caption{Actual Labels}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/k_means_across_media_predicted.png}
        \caption{K Means Clustering (misclassifications in bold)}
        \label{fig:subfig2}
    \end{subfigure}
    \label{fig:main}
\end{figure} 

Some hypotheses about the misclassifications:

1. Wealthy power plant owner Mr. Burns is misclassified as being in Downtown Abbey (instead of the Simpsons), a show that depicts elite society in Victorian England.

2. Aristocratic (and arrogant) Mr. Darcy is misclassified as being in The Lord of the Rings (instead of Pride and Prejudice) for having a similar vocabulary to the wizard Gandalf.

3. Mr. Carson is misclassified as being part of Pride and Prejudice (instead of Downtown Abbey), for presumably speaking more formally by virtue of being the head butler.

\subsection{Classification}

A Naive Bayes Classifier was chosen because of the model's efficient training on sparse matrices, assumed independence between features, and a track record of performing well on NLP tasks^4.

It became apparent that it was necessary to tune the model's hyper parameters.

1. Use over-sampling to balance the dataset, so the classifier does not bias against an under-represented media or speaker in the corpus.

2. Extended the n-gram range from one to three to capture short expressions and catch-phrases in addition to individual words.

3. Lowered the maximum document frequency from 1 to 0.99, removing the noise of any word or phrase that is present in every speaker's vocabulary.

The final model:

\begin{verbatim}
model = make_pipeline(
    TfidfVectorizer(ngram_range=(1, 3), max_df=0.99),
    SMOTE(),
    MultinomialNB()
)
\end{verbatim}

\section{Results}
This is the results section.

\section{Conclusion}
This is the conclusion section.

\section{Future Work}

\section{References}

\section{Appendix}
\subsubsection{Major Code Sections}
\textbf{Data Preparation}: \url{github.com/wluna01/isye-6740-project/blob/main/code/data_prep.py}

\textbf{Heuristics}: \url{github.com/wluna01/isye-6740-project/blob/main/code/heuristics.py}

\textbf{Stylometry Feature Extraction}: \url{github.com/wluna01/isye-6740-project/blob/main/code/data_feature_extraction.py}

\textbf{Stylometry Classifier}: \url{github.com/wluna01/isye-6740-project/blob/main/code/stylometry_classifier.py}

\subsubsection{References}
[1] \url{direct.mit.edu/coli/article/41/3/481/1519/Computational-Constancy-Measures-of-Texts-Yule-s-K}

[2] \url{statology.org/simpsons-diversity-index/}

[3] \url{https://www.jstor.org/stable/30204514}

[4] \url{web.archive.org/web/20180604105217id_/http://onlinepresent.org/proceedings/vol111_2015/50.pdf}

BREAKKKKKKK




\begin{figure}[H]
\centering
\includegraphics[width=\largeimagewidth]{images/heuristics.png}
\caption{Speakers within a piece of media tend to cluster together.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\mediumimagewidth]{images/Downtown Abbey_heuristics.png}
\caption{Note that the three servants are clustered in the center while each member of the nobility occupies a distinct place in the plot.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\mediumimagewidth]{images/The Lord of the Rings_heuristics.png}
\caption{This scatter plot implies that Frodo and Sam have similar speech patterns, as do Pippin and Mary, distinct from Gandalf and Aragorn.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\mediumimagewidth]{images/The Simpsons_heuristics.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\mediumimagewidth]{images/Pride and Prejudice_heuristics.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\mediumimagewidth]{images/Friends_heuristics.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\mediumimagewidth]{images/Rick and Morty_heuristics.png}
\end{figure}


    \begin{figure}[H]
        \centering
        \caption{Decision Stumps of AdaBoost (Misclassifications of each stump highlighted)}
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/Friends_heuristics.png}
            \caption{T = 1}
            \label{fig:subfig1}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/Downtown Abbey_heuristics.png}
            \caption{T = 2}
            \label{fig:subfig2}
        \end{subfigure}
        \label{fig:main}
    \end{figure} 
        { \Large \bfseries Project Proposal}

        \begin{itemize}
            \item[] \textbf{Problem Statement}
            
			

            \item[] \textbf{Data Sources}
            

        \item[] \textbf{Methodology}
            
		Collect representations of dialogue that capture sufficient variability to distinguish speakers:
		
		\begin{enumerate}
		
			\item Vocabulary
			\begin{itemize}
				\item For each speaker, calculate the Term Frequency-Inverse Document Frequency (TF-IDF), where Document Frequency is based on the corpus of all dialogue of the show or film the character is present in, and store as a matrix.
				\item Include bigrams and trigrams in the matrix to capture use of multi-word expressions and catchphrases.
				\item Take the Cosine Similarity between each speaker vector with every other speaker vector in the same piece of media, plotting as a heatmap.
			\end{itemize}
		
			\item Stylometry
			\begin{itemize}
				\item For each speaker, calculate their distribution of word length and sentence length of all dialogue spoken.
				\begin{itemize}
					\item How to best represent the distribution will be a result of analyzing the distributions, although most dialogue has a skewed unimodal distribution (lots of short words and sentences, a few longer ones) that can be sufficiently captured by mean, standard deviation, and skewness.
				\end{itemize}
				\item For each speaker, calculate the frequency of their use of function words and common punctuation, again plotting Cosine Similarity as a heatmap.
			\end{itemize}
		
			\item Visualize
			\begin{itemize}
				\item Create a scatter plot showing the Cosine Similarity of the Vocabulary Matrix on one axis and stylometry on the other.
				\begin{itemize}
					\item Generate Heatmaps that use different measures of distance (Manhattan, Euclidean, etc.) and consider pros and cons of adopting them as alternatives.
				\end{itemize}
			\end{itemize}
		\end{enumerate}
	
		\item[] \textbf{Evaluation}

		Within-Media

		\begin{enumerate}
			\item For each media (i.e. movie or show), train a Naive Bayes classifier on the Vocabulary and Stylometry matrices.
			\item Interpret the results. Which media has dialogue that the Bayesian Classifier can most effectively predict?
		\end{enumerate}

		Across-Media

		\begin{enumerate}
			\item Perform PCA on the TF-IDF vectors that represent each speaker with each media.
			\item Perform K-means Clustering on the top 2-3 components of the resulting dataset, where $k$ equals the number of distinct tv shows and movies. Calculate the purity score where ``each cluster is assigned to the class which is most frequent in the cluster'' (taken verbatim from HW1).
			\item Interpret the results. Which clusters have the highest and lowest purity scores?
		\end{enumerate}
		

\item[] \textbf{Applications of Coursework}

- \textbf{Naive Bayes} to train a classifier to predict a speaker within a specific piece of media

- \textbf{Minkowski Distances} (Euclidean, Manhattan, etc)

- \textbf{Principal Component Analysis} to prepare TF-IDF vectors for clustering

- \textbf{K-Means Clustering} to classify speakers of dialogue across media

\item[] \textbf{Background}

Can, F., and Patton, J. M. (2004). Change of Writing Style with Time. Computers and the Humanities, 38(1), 61 82. http://www.jstor.org/stable/30204925

Kumiko Tanaka-Ishii, Shunsuke Aihara; Computational Constancy Measures of Texts—Yule's K and Rényi's Entropy. Computational Linguistics 2015; 41 (3): 481 502. Link.

Simpson, E. Measurement of Diversity. Nature 163, 688 (1949). Link. 

Spärck Jones, K. (2021). A statistical interpretation of term specificity and its application in retrieval. J. Documentation, 60, 493-502.

Tweedie, F. J., Singh, S., and Holmes, D. I. (1996). Neural Network Applications in Stylometry: The “Federalist Papers.” Computers and the Humanities, 30(1), 1 10. http://www.jstor.org/stable/30204514
   

    \end{itemize}
	
	\pagebreak

\end{titlepage}


\end{document}

